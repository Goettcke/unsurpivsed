points(xx,reg.summary$bic[xx], col="blue",cex=2,pch=20)
plot(reg.summary$adjr2,xlab="Number of Predictors",ylab=bquote(R^2: .("Adjusted")),type="l",col="red")
title("NYC Taxi data: Exhaustive")
xx                   = which.max(reg.summary$adjr2)
points(xx,reg.summary$adjr2[xx], col="blue",cex=2,pch=20)
# Forward Stepwise Selection
regfit.fwd           = regsubsets(formula.nyctaxi,data = train_group,nbest = 1,nvmax = 20,method = c("forward"))
summary(regfit.fwd)
# Backward Stepwise Selection
regfit.bwd           = regsubsets(formula.nyctaxi,data = train_group,nbest = 1,nvmax = 20,method = c("backward"))
summary(regfit.bwd)
# Comparison
coef(regfit.full,8)
coef(regfit.fwd,8)
coef(regfit.bwd,8)
# Ridge Regression
x        = model.matrix(formula.nyctaxi,data = train_group)[,-1]
y        = train_group$tgroup
ridge.mod = glmnet(x,y,family = c("binomial"),alpha=0)
par(mfrow=c(1,2))
plot(ridge.mod, xvar = "lambda")
plot(ridge.mod, xvar = "norm")
cv.ridge  = cv.glmnet(x,y,family = c("binomial"),alpha=0)
plot(cv.ridge)
coef(cv.ridge,s=cv.ridge$lambda.1se)
# Ridge, Lasso or OLS: Which is best for predicting y.test based on the train data (x.train,y.train) and test data (x.test,y.test) as defined below? Are the results sensitive to the choice of training and test data?
x.train = model.matrix(formula.nyctaxi,data = train_group[training.data,])[,-1]
y.train = train_group$tgroup[training.data]
x.test  = model.matrix(formula.nyctaxi,data = train_group[-training.data,])[,-1]
y.test  = train_group$tgroup[-training.data]
# Pick the best OLS classifier by subsetting (using the training data only) and report classification error based on the test data
# ...your code here...
# Exhaustive Selection
regfit.full          = regsubsets(formula.nyctaxi,data = train_group,nbest = 1,nvmax = 20,method = c("exhaustive"))
summary(regfit.full)
reg.summary          = summary(regfit.full)
par(mfrow=c(1,2))
plot(reg.summary$rss,xlab="Number of Variables",ylab="Residual Sum of Squares",type="l",col="red")
title("NYC Taxi data: Exhaustive")
xx                   = which.min(reg.summary$rss)
points(xx,reg.summary$rss[xx], col="blue",cex=2,pch=20)
plot(reg.summary$rsq,xlab="Number of Variables",ylab=expression(R^2),type="l",col="red")
title("NYC Taxi data: Exhaustive")
xx                   = which.max(reg.summary$rsq)
points(xx,reg.summary$rsq[xx], col="blue",cex=2,pch=20)
par(mfrow=c(1,3))
plot(reg.summary$cp,xlab="Number of Predictors",ylab=expression(Cp),type="l",col="red")
title("NYC Taxi data: Exhaustive")
xx                   = which.min(reg.summary$cp)
points(xx,reg.summary$cp[xx], col="blue",cex=2,pch=20)
plot(reg.summary$bic,xlab="Number of Predictors",ylab=expression(BIC),type="l",col="red")
title("NYC Taxi data: Exhaustive")
xx                   = which.min(reg.summary$bic)
points(xx,reg.summary$bic[xx], col="blue",cex=2,pch=20)
plot(reg.summary$adjr2,xlab="Number of Predictors",ylab=bquote(R^2: .("Adjusted")),type="l",col="red")
title("NYC Taxi data: Exhaustive")
xx                   = which.max(reg.summary$adjr2)
points(xx,reg.summary$adjr2[xx], col="blue",cex=2,pch=20)
# Forward Stepwise Selection
regfit.fwd           = regsubsets(formula.nyctaxi,data = train_group,nbest = 1,nvmax = 20,method = c("forward"))
summary(regfit.fwd)
# Backward Stepwise Selection
regfit.bwd           = regsubsets(formula.nyctaxi,data = train_group,nbest = 1,nvmax = 20,method = c("backward"))
summary(regfit.bwd)
# Comparison
coef(regfit.full,8)
coef(regfit.fwd,8)
coef(regfit.bwd,8)
# Pick the best Ridge classifier (using the training data only) and report classification error based on the test data
# ...your code here...
# Pick the best Lasso classifier (using the training data only) and report classification error based on the test data
# ...your code here...
regfit.bwd          = regsubsets(formula.nyctaxi,data = train_group,nbest = 1,nvmax = 20,method = c("backwards"))
regfit.bwd          = regsubsets(formula.nyctaxi,data = train_group,nbest = 1,nvmax = 20,method = c("backward"))
reg.summary          = summary(bwd)
regfit.bwd          = regsubsets(formula.nyctaxi,data = train_group,nbest = 1,nvmax = 20,method = c("backward"))
reg.summary          = summary(regfit.bwd)
reg.summary          = summary(regfit.bwd)
summary(regfit.bwd
)
summary(regfit.full)
summary(regfit.bwd) == summary(regfit.full)
regfit.fwd          = regsubsets(formula.nyctaxi,data = train_group,nbest = 1,nvmax = 20,method = c("forward"))
reg.summary          = summary(fwd)
reg.summary          = summary(regfit.fwd)
summary(regfit.bwd)
summary(regfit.fwd)
knitr::opts_chunk$set(echo = TRUE)
packlist = c("plyr","dplyr","readr","data.table",
"tibble","lubridate","geosphere","forcats",
"ggplot2","caret","Metrics","DAAG",
"rattle","leaps","MASS","ROCR","pROC","ISLR","glmnet","tree","randomForest","gbm")
install.packages(packlist)
install.packages(packlist)
packlist = c("plyr","dplyr","readr","data.table",
"tibble","lubridate","geosphere","forcats",
"ggplot2","caret","Metrics","DAAG",
"rattle","leaps","MASS","ROCR","pROC","ISLR","glmnet","tree","randomForest","gbm")
install.packages(packlist)
install.packages('e1071', dependencies=TRUE)
install.packages(packlist)
install.packages(packlist)
knitr::opts_chunk$set(echo = TRUE)
packlist = c("plyr","dplyr","readr","data.table",
"tibble","lubridate","geosphere","forcats",
"ggplot2","caret","Metrics","DAAG",
"rattle","leaps","MASS","ROCR","pROC","ISLR","glmnet","tree","randomForest","gbm")
install.packages(packlist)
install.packages('e1071', dependencies=TRUE)
install.packages(packlist)
install.packages(packlist)
install.packages(packlist)
library('plyr')
packlist = c("plyr","dplyr","readr","data.table",
"tibble","lubridate","geosphere","forcats",
"ggplot2","caret","Metrics","DAAG",
"rattle","leaps","MASS","ROCR","pROC","ISLR","glmnet","tree","randomForest","gbm")
install.packages(packlist)
install.packages(packlist)
library('plyr')
library('dplyr') # data manipulation
library('readr') # input/output
library('data.table') # data manipulation
library('tibble') # data wrangling
library('lubridate') # date and time
packlist = c("plyr","dplyr","readr","data.table",
"tibble","lubridate","geosphere","forcats",
"ggplot2","caret","Metrics","DAAG",
"rattle","leaps","MASS","ROCR","pROC","ISLR","glmnet","tree","randomForest","gbm")
install.packages(packlist)
install.packages('e1071', dependencies=TRUE)
library('plyr')
install.packages(packlist)
packlist = c("plyr","dplyr","readr","data.table",
"tibble","lubridate","geosphere","forcats",
"ggplot2","caret","Metrics","DAAG",
"rattle","leaps","MASS","ROCR","pROC","ISLR","glmnet","tree","randomForest","gbm")
install.packages(packlist)
install.packages('e1071', dependencies=TRUE)
library('plyr')
knitr::opts_chunk$set(echo = TRUE)
load(file = "train_external_data_cleaned.Rdata")
train$wday  = factor(train$wday,ordered=F)
train$month = factor(train$month,ordered=F)
train_group <- train %>%
mutate(excess_duration = trip_duration-OSRM_travel_time,
tgroup = case_when(excess_duration < 25 ~ "fast",
excess_duration >= 25 & excess_duration <= 1.6e3 ~ "mid",
excess_duration > 1.6e3 ~ "slow"),
tgroup = factor(tgroup,levels=c("slow","mid","fast")))
knitr::opts_chunk$set(echo = TRUE)
packlist = c("plyr","dplyr","readr","data.table",
"tibble","lubridate","geosphere","forcats",
"ggplot2","caret","Metrics","DAAG",
"rattle","leaps","MASS","ROCR","pROC","ISLR","glmnet","tree","randomForest","gbm")
install.packages(packlist)
install.packages('e1071', dependencies=TRUE)
library('plyr')
packlist = c("plyr","dplyr","readr","data.table",
"tibble","lubridate","geosphere","forcats",
"ggplot2","caret","Metrics","DAAG",
"rattle","leaps","MASS","ROCR","pROC","ISLR","glmnet","tree","randomForest","gbm")
install.packages(packlist)
install.packages('e1071', dependencies=TRUE)
library('dplyr') # data manipulation
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
packlist = c("plyr","dplyr","readr","data.table",
"tibble","lubridate","geosphere","forcats",
"ggplot2","caret","Metrics","DAAG",
"rattle","leaps","MASS","ROCR","pROC","ISLR","glmnet","tree","randomForest","gbm")
install.packages(packlist)
install.packages('e1071', dependencies=TRUE)
library('plyr')
library('plyr')
library("plyr")
install.packages()
install.packages("plyr")
library(plyr)
d <- data.frame(list(metalbands=c("Nile","Cattle Decapitation", "Aborted"), awesomenessfactor=c(42,45,44)))
d
setwd("C:/Users/jogoe12/Dropbox/Uni/Datalogi/7.semester/Parallel\ Computing/Assignment\ 1/")
x <- c(1,-2,3,4,5)
mean(x)
x <- c(1,-2,3,4,5)
mean(x)
max(x)
min(x)
mean(abs(x))
x <- c(x[1:2],42,x[3:4])
x
y <- c(1,2,3,4,5,6)
z <- x+y
z
x <- c(1,-2,3,4,5)
mean(x)
max(x)
min(x)
x <- c(x[1:2],42,x[3:4])
x
x
y <- c(1,2,3,4,5)
z <- x+y
z
x <- rnorm(100)
help(rnorm)
x[(length(x)-4):length(x)]
mean(x)
m <- rbind(c(1,1),c(2,2))
m[1,1]
m
y <- rbind(c(-1,-1),c(-2,-2))
z <- m+y
z
n <- rbind(c(2,0),c(0,2))
y <- m%*%n
y
n
y <- m%*%n
y
help(AirPassengers)
hist(AirPassengers)
AirPassengers
plot(AirPassengers)
class(AirPassengers)
plot(AirPassengers)
mosaicplot(Titanic)
class(Titanic)
hist(Titanic)
mosaicplot(Titanic)
irisCluster <- kmeans(iris[, 3:4], 3, nstart = 20)
irisCluster
iris[,3:4]
library(ggplot2)
ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()
teachers <- c("Claus Meer","Bjarne Toft", "SÃ¸ren Haagerup")
cupsofcoffeeprday <- c(10,3,42)
hoursofworkprday <- c(22,0,22)
d <- data.frame(teachers,cupsofcoffeeprday,hoursofworkprday, stringsAsFactors = FALSE)
d$teachers=="Bjarne Toft"
d$teachers[cupsofcoffeeprday > 10]
x <- c(1,-2,3,4,5)
x
mean(x)
max(x)
min(x)
mean(abs(x))
x <- c(x[1:2],42,x[3:4])
x
x <- c(x[1:2],42,x[4:5])
x
x <- c(1,-2,3,4,5)
x
x <- c(x[1:2],42,x[4:5])
x
x[3] <- 43
x
y <- c(1,2,3,4,5)
z <- x+y
z
x <- rnorm(100)
help(rnorm)
x[(length(x)-4):length(x)]
mean(x)
m <- rbind(c(1,1),c(2,2))
m[1,1]
m
x[-1:-5]
x[(length(x)-4):length(x)]
x[-1:-5]
x[(length(x)-4):length(x)]
x[-1:-5]
x[(length(x)-4):length(x)]
x[-5]
x[(length(x)-4):length(x)]
mean(x)
n <- rbind(c(2,0),c(0,2))
n
y <- m%*%n
y
AirPassengers
hist(AirPassengers)
plot(AirPassengers)
class(AirPassengers)
mode(AirPassengers)
mosaicplot(Titanic)
class(Titanic)
hist(Titanic)
mosaicplot(Titanic)
teachers <- c("Claus Meer","Bjarne Toft", "SÃ¸ren Haagerup")
cupsofcoffeeprday <- c(10,3,42)
hoursofworkprday <- c(22,0,22)
d <- data.frame(teachers,cupsofcoffeeprday,hoursofworkprday, stringsAsFactors = FALSE)
d$teachers=="Bjarne Toft"
d$teachers[cupsofcoffeeprday > 10]
p <- c(0.5,1)
library(ggplot2)
dataset <- fread("uk.dat")
require("knitr")
knitr::opts_chunk$set(echo=TRUE, error=FALSE)
opts_knit$set(root.dir = dirname(rstudioapi::getActiveDocumentContext()$path))
library(data.table)
library(ggplot2)
dataset <- fread("uk.dat")
getdir()
getwd()
knitr::opts_chunk$set(echo=TRUE, error=FALSE)
opts_knit$set(root.dir = dirname(rstudioapi::getActiveDocumentContext()$path))
getwd()
getwd()
opts_knit$set(root.dir = dirname(rstudioapi::getActiveDocumentContext()$path))
getwd()
library("rstudioapi")
require("knitr")
knitr::opts_chunk$set(echo=TRUE, error=FALSE)
opts_knit$set(root.dir = dirname(rstudioapi::getActiveDocumentContext()$path))
getwd()
setwd("D:\OneDrive - Syddansk Universitet\uni\datalogi\8.semester\Unsupervised Learning\exercise2")
setwd("D:\OneDrive\ -\ Syddansk Universitet\uni\datalogi\8.semester\Unsupervised\ Learning\exercise2")
setwd("D:\")
require("knitr")
knitr::opts_chunk$set(echo=TRUE, error=FALSE)
opts_knit$set(root.dir = dirname(rstudioapi::getActiveDocumentContext()$path))
library(data.table)
# Exercise Sheet 2 - Unsupervised Learning
# Exercise solution skeleton
# For plotting we will make use of the ggplot2 library
library(ggplot2)
dataset <- fread("uk.dat")
# to inspect function description use ?func
# 1. PCA
# (a) Calculate a centered version of the dataset X
# read in ./uk.dat as numercial matrix and assign row names ~ 3 line of code
# to center a matrix, use scale() - it requries a - numeric - matrix as imput (see ?scale ), but centers columns wise - we have rows of values!
# ~1 line of code
# (b) Calculate the Covariance matrix C
# The covariance matrix Q of a centered matrix M is defined as Q = 1/(n-1)*(M,M^T)
# where n is the number of columns in M and M^T is the transposed matrix M
# ~ 1 line of code
# (c) Calculate the Eigenvalues and Eigenvectors of C
# use builtin function eigen() to compute the eigenvalues and vectors. You can separate them by using the accessor $values and $vectors
# ~ 3 lines of code
# (d) Calculate and plot the coordinates of our 4 objects using
# i  - The first principal component
# ii - The first and second principal component
# For this we need to extract the first and second eigenvector, use cbind() to form them back into a matrix
# ~ 2 lines of code
# Multiply the eigenvector with the centerd matrix  to get the values of the pricipal components
# ~ 2 lines of code
# plot by country
# plot the coordinates of the first principal component - use qplot - quickplot from ggplot2
# ~ 5 lines of code
# plot the coordinates of the first two principal components
# ~ 4 lines of code
# plot by cosumable
# First PC
# First 2 PC
# ===================================================================
# 2. Multidimensional Scaling
# (a) Calculated the Matrix B as explained in the lecture
# B = X^T x X
# We provide two datasets, denmark.dat and germany.dat
# drop the X column for the matrix again and assign row names
# convert the distances matrix into B as described in the slides (p. 57)
# A = (a_ij) = -1/2* d_ij ^2
# ~ 1 line of code
# Calculate the average of the row in A, the average of the columns in A and the average of the matrix
# You can use the function mean in combination with apply () - see ?apply for rowwise and column wise application
# ~ 3 lines of code
# define B = (b_ij) as b_ij = a_ij - a_i[row] - a_j[column] + mean(A)
# where a_i[row] is the average of row i, a_j[column] is the average of column j and mean(A) the average of A
# First copy dimensions of A to B, then overwrite all values
# your for loop here
# (b) Calculate the Eigenvalues and Eigenvectors of B
# as before
# ~ 2 lines of code
# (c) Use the first two Eigenvectors of B to calculate the tranformed coordinates of the cities
# As before use cbind to assign and multiply with B
# ~ 2 lines of code
# plot the original data
# fimd a rotation facor alpha_denmark and alpha_germany that rotates the points in the regular projection (e.g. 0.3*pi)
#Rotation Denmark
#Rotation Germany
# To apply rotation, we multiply with a 2x2 matrix:
# 2x2 matrix:
# cos(alpha), sin(alpha)
# -sin(alpha), cos(alpha)
# plot the result again with qplot()
# alpha = alpha_denmark
# alpha = alpha_germany
# rotation.M = matrix(c(cos(alpha_denmark), sin(alpha_denmark), -sin(alpha_denmark), cos(alpha_denmark)), ncol=2)
setwd("D:\")
require("knitr")
knitr::opts_chunk$set(echo=TRUE, error=FALSE)
opts_knit$set(root.dir = dirname(rstudioapi::getActiveDocumentContext()$path))
library(data.table)
# Exercise Sheet 2 - Unsupervised Learning
# Exercise solution skeleton
# For plotting we will make use of the ggplot2 library
library(ggplot2)
dataset <- fread("uk.dat")
# to inspect function description use ?func
# 1. PCA
# (a) Calculate a centered version of the dataset X
# read in ./uk.dat as numercial matrix and assign row names ~ 3 line of code
# to center a matrix, use scale() - it requries a - numeric - matrix as imput (see ?scale ), but centers columns wise - we have rows of values!
# ~1 line of code
# (b) Calculate the Covariance matrix C
# The covariance matrix Q of a centered matrix M is defined as Q = 1/(n-1)*(M,M^T)
# where n is the number of columns in M and M^T is the transposed matrix M
# ~ 1 line of code
# (c) Calculate the Eigenvalues and Eigenvectors of C
# use builtin function eigen() to compute the eigenvalues and vectors. You can separate them by using the accessor $values and $vectors
# ~ 3 lines of code
# (d) Calculate and plot the coordinates of our 4 objects using
# i  - The first principal component
# ii - The first and second principal component
# For this we need to extract the first and second eigenvector, use cbind() to form them back into a matrix
# ~ 2 lines of code
# Multiply the eigenvector with the centerd matrix  to get the values of the pricipal components
# ~ 2 lines of code
# plot by country
# plot the coordinates of the first principal component - use qplot - quickplot from ggplot2
# ~ 5 lines of code
# plot the coordinates of the first two principal components
# ~ 4 lines of code
# plot by cosumable
# First PC
# First 2 PC
# ===================================================================
# 2. Multidimensional Scaling
# (a) Calculated the Matrix B as explained in the lecture
# B = X^T x X
# We provide two datasets, denmark.dat and germany.dat
# drop the X column for the matrix again and assign row names
# convert the distances matrix into B as described in the slides (p. 57)
# A = (a_ij) = -1/2* d_ij ^2
# ~ 1 line of code
# Calculate the average of the row in A, the average of the columns in A and the average of the matrix
# You can use the function mean in combination with apply () - see ?apply for rowwise and column wise application
# ~ 3 lines of code
# define B = (b_ij) as b_ij = a_ij - a_i[row] - a_j[column] + mean(A)
# where a_i[row] is the average of row i, a_j[column] is the average of column j and mean(A) the average of A
# First copy dimensions of A to B, then overwrite all values
# your for loop here
# (b) Calculate the Eigenvalues and Eigenvectors of B
# as before
# ~ 2 lines of code
# (c) Use the first two Eigenvectors of B to calculate the tranformed coordinates of the cities
# As before use cbind to assign and multiply with B
# ~ 2 lines of code
# plot the original data
# fimd a rotation facor alpha_denmark and alpha_germany that rotates the points in the regular projection (e.g. 0.3*pi)
#Rotation Denmark
#Rotation Germany
# To apply rotation, we multiply with a 2x2 matrix:
# 2x2 matrix:
# cos(alpha), sin(alpha)
# -sin(alpha), cos(alpha)
# plot the result again with qplot()
# alpha = alpha_denmark
# alpha = alpha_germany
# rotation.M = matrix(c(cos(alpha_denmark), sin(alpha_denmark), -sin(alpha_denmark), cos(alpha_denmark)), ncol=2)
setwd("D:\OneDrive\ -\ Syddansk\ Universitet")
x <- c(1,2)
x
clear
ls
library(ggplot2)
library(plyr)
library(cluster)
library(gtools)
version
library(ggplot2)
library(plyr)
library(cluster)
library(gtools)
library(ClusterR)
library(stringi)
require(graphics)
## EXPECTATION STEP ##
# Compute the expected classes for all data points of the two Gaussian distributions
x.g1
# Create Our dataset
x.g1 <- rnorm(250, 1, .8)
x.g2 <- rnorm(150, 4, .8)
X <- sort(c(x.g1,x.g2))
hist(X)
## EXPECTATION STEP ##
# Compute the expected classes for all data points of the two Gaussian distributions
x.g1
x.g2
?rnorm
mean(x.g2)
mean(x.g1)
?do.call
?data.table
?apply
data
?ruspini
?data
?silhouette
library(cluster)
?silhouette
matrix <- (c(c(1,2),c(2,3)))
matrix <- matrix(c(c(1,2),c(2,3)))
matrixx <- matrix(c(c(1,2),c(2,3)))
matrixx
?matrix
install.packages('transclustr.tar.gz', repos=NULL,type='source')
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
install.packages('transclustr.tar.gz', repos=NULL,type='source')
install.packages('transclustr.tar.gz', repos=NULL,type='source')
